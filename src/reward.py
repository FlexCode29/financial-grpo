import re
from typing import List, Dict

# Define the ordered bands for "near miss" calculation.
# A "near miss" is a prediction that is one step away.
ORDERED_BANDS = {
    "returns":    ["bad", "neutral", "good"],
    "volatility": ["low", "medium", "high"],
}

def _parse_prediction(completion: str, tag: str) -> str:
    """
    Parses a single predicted value from the model's XML-like output.
    Example: Searches for <returns_1y>good</returns_1y> and returns "good".
    """
    pattern = f"<{tag}>(.*?)</{tag}>"
    match = re.search(pattern, completion, re.DOTALL)
    if match:
        return match.group(1).strip().lower()
    return None

def _calculate_score(predicted: str, actual: str, band_type: str) -> float:
    """
    Calculates the reward score for a single prediction based on user-defined rules.
    """
    if predicted is None or actual is None:
        return -2.0 # Harsh penalty for not providing a prediction in the right format.

    if predicted not in ORDERED_BANDS[band_type]:
        return -2.0 # Harsh penalty for hallucinating a non-existent band.

    if predicted == actual:
        return 1.0  # Perfect match

    # Check for a "near miss"
    try:
        predicted_idx = ORDERED_BANDS[band_type].index(predicted)
        actual_idx = ORDERED_BANDS[band_type].index(actual)
        if abs(predicted_idx - actual_idx) == 1:
            return 0.1  # Near miss
    except ValueError:
        # This case should be caught by the check above, but as a safeguard:
        return -2.0

    return -1.0  # Bad miss (e.g., 'good' vs 'bad')

def financial_reward_function(
    prompts: List[str], completions: List[str], **kwargs
) -> List[float]:
    """
    The main GRPO reward function for the dual-objective financial task.
    
    This function is called by the TRL GRPOTrainer with a batch of prompts,
    completions, and the corresponding ground-truth data from the dataset.
    
    Args:
        prompts: The input prompts sent to the model.
        completions: The text generated by the model.
        **kwargs: A dictionary containing the ground-truth columns from the dataset,
                  e.g., kwargs['ground_truth_returns_1y'], etc.

    Returns:
        A list of float rewards, one for each completion.
    """
    
    # The TRL trainer conveniently passes all columns from the dataset via kwargs.
    # Let's get the ground-truth labels for the whole batch.
    actuals_returns_1y = kwargs["ground_truth_returns_1y"]
    actuals_volatility_1y = kwargs["ground_truth_volatility_1y"]
    actuals_returns_5y = kwargs["ground_truth_returns_5y"]
    actuals_volatility_5y = kwargs["ground_truth_volatility_5y"]
    
    batch_rewards = []
    
    # Iterate through each completion in the batch
    for i, enumerate_completions in enumerate(completions):
        completion = enumerate_completions[0]['content']

        # --- 1. Parse Predictions ---
        # We need to find the model's answers within its generated text.
        pred_returns_1y = _parse_prediction(completion, "returns_1y")
        pred_volatility_1y = _parse_prediction(completion, "volatility_1y")
        pred_returns_5y = _parse_prediction(completion, "returns_5y")
        pred_volatility_5y = _parse_prediction(completion, "volatility_5y")
        
        # --- 2. Score Each Prediction ---
        # Calculate the score for each of the four prediction tasks.
        score_ret_1y = _calculate_score(pred_returns_1y, actuals_returns_1y[i], "returns")
        score_vol_1y = _calculate_score(pred_volatility_1y, actuals_volatility_1y[i], "volatility")
        score_ret_5y = _calculate_score(pred_returns_5y, actuals_returns_5y[i], "returns")
        score_vol_5y = _calculate_score(pred_volatility_5y, actuals_volatility_5y[i], "volatility")
        
        # --- 3. Combine Scores for Total Reward ---
        # The total reward for this one completion is the sum of individual scores.
        # The theoretical max reward is 1.0 * 4 = 4.0
        # The theoretical min reward is -2.0 * 4 = -8.0
        total_reward = score_ret_1y + score_vol_1y + score_ret_5y + score_vol_5y
        batch_rewards.append(total_reward)
        
    return batch_rewards